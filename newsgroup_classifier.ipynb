{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Due to Memory Constraints, Only Four Categories of News Group are used.\n",
      "===== categories : ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med'] ======\n",
      "===== Loading Partial Training Dataset =====\n",
      "===== Loading Partial Testing Dataset =====\n",
      "===== PreProcessing =====\n",
      " ========================= \n",
      " twenty_train_partial.data size : 2257\n",
      " text_sents_clean size : 225\n",
      "===== Features =====\n",
      "===== Bag of Words =====\n",
      "Written to bag_of_words.txt\n",
      "===== N-grams =====\n",
      "Written to n-grams.txt\n",
      "===== TFIDF =====\n",
      "Written to tfidf.txt\n",
      "===== POS-tags =====\n",
      "Written to POS-tags.txt\n",
      "===== Lemmatization (Head Words) =====\n",
      "Written to HeadWords.txt\n",
      " ========================= \n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "import time\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import math\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.util import ngrams\n",
    "\n",
    "categories = ['alt.atheism', 'soc.religion.christian',\n",
    "               'comp.graphics', 'sci.med']\n",
    "\n",
    "print(\"===== Due to Memory Constraints, Only Four Categories of News Group are used.\")\n",
    "print(\"===== categories : \"+str(categories)+\" ======\")\n",
    "\n",
    "print(\"===== Loading Partial Training Dataset =====\")\n",
    "twenty_train_partial = fetch_20newsgroups(subset='train', \n",
    "remove=('headers', 'footers'\n",
    "#         , 'quotes'\n",
    "       ), \n",
    "                                          categories=categories, shuffle=True, random_state=42)\n",
    "\n",
    "print(\"===== Loading Partial Testing Dataset =====\")\n",
    "twenty_test_partial = fetch_20newsgroups(subset='test', \n",
    "remove=('headers', 'footers'\n",
    "#         , 'quotes'\n",
    "       ), \n",
    "                                         categories=categories, shuffle=True, random_state=42)\n",
    "\n",
    "print(\"===== PreProcessing =====\")\n",
    "\n",
    "def remove_string_special_characters(s):\n",
    "    stripped = re.sub('_+', '', s)\n",
    "    stripped = re.sub('\\s+', ' ', stripped)\n",
    "    stripped = re.sub('\\d', '', stripped)\n",
    "    stripped = re.sub('/', ' ', stripped)\n",
    "    stripped = re.sub('[^\\w\\s]', '', stripped)\n",
    "    stripped = stripped.lower()\n",
    "    return stripped\n",
    "\n",
    "text_sents_clean = []\n",
    "\n",
    "def main():\n",
    "    q = multiprocessing.Queue()\n",
    "    for i in range(int(len(twenty_train_partial.data)/10)):\n",
    "        s=remove_string_special_characters(twenty_train_partial.data[i])\n",
    "        text_sents_clean.append(s)\n",
    "        q.put(i)\n",
    "    time.sleep(0.1)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "print(\" ========================= \")\n",
    "print(\" twenty_train_partial.data size : \"+str(len(twenty_train_partial.data)))\n",
    "print(\" text_sents_clean size : \"+str(len(text_sents_clean)))\n",
    "print(\"===== Features =====\")\n",
    "\n",
    "print(\"===== Bag of Words =====\")\n",
    "count_vect = CountVectorizer(analyzer='word', stop_words='english')\n",
    "X_train_counts = count_vect.fit_transform(text_sents_clean)\n",
    "list_bow = list(count_vect.vocabulary_.keys())[:]\n",
    "\n",
    "with open('bag_of_words.txt', 'w') as f:\n",
    "    for item in list_bow:\n",
    "        f.write('%s\\n' % item)\n",
    "print(\"Written to bag_of_words.txt\")\n",
    "\n",
    "print(\"===== N-grams =====\")\n",
    "\n",
    "output = list()\n",
    "\n",
    "for s in text_sents_clean:\n",
    "\n",
    "    tokens = [token for token in s.split(\" \") if token != \"\"]\n",
    "    output += list(ngrams(tokens, 3))\n",
    "\n",
    "with open('n-grams.txt', 'w') as f:\n",
    "    for item in output:\n",
    "        f.write('%s\\n' % str(item))\n",
    "print(\"Written to n-grams.txt\")\n",
    "\n",
    "print(\"===== TFIDF =====\")\n",
    "\n",
    "def get_doc(sent):\n",
    "    doc_info = []\n",
    "    i=0\n",
    "    for sent in text_sents_clean:\n",
    "        i += 1\n",
    "        count=count_words(sent)\n",
    "        temp={'doc_id' : i, 'doc_length' : count }\n",
    "        doc_info.append(temp)\n",
    "    return doc_info\n",
    "\n",
    "def count_words(sent):\n",
    "    count = 0\n",
    "    words = word_tokenize(sent)\n",
    "    for word in words:\n",
    "        count += 1\n",
    "    return count\n",
    "\n",
    "def create_freq_dicts(sents):\n",
    "    i=0\n",
    "    freqDict_list = []\n",
    "    for sent in sents:\n",
    "        i+=1\n",
    "        freq_dict = {}\n",
    "        words = word_tokenize(sent)\n",
    "        for word in words:\n",
    "#             word = word.lower()\n",
    "            if word in freq_dict:\n",
    "                freq_dict[word]=+1\n",
    "            else:\n",
    "                freq_dict[word]=1\n",
    "#             temp={'doc_id' : i, 'freq_dict' : freq_dict }\n",
    "        temp={'doc_id' : i, 'freq_dict' : freq_dict }\n",
    "        freqDict_list.append(temp)\n",
    "    return freqDict_list\n",
    "\n",
    "def computeTF(doc_info, freqDict_list):\n",
    "    TF_scores=[]\n",
    "    for tempDict in freqDict_list:\n",
    "        id=tempDict['doc_id']\n",
    "        for k in tempDict['freq_dict']:\n",
    "            temp={'doc_id' : id, \n",
    "                  'TF_score' : tempDict['freq_dict'][k]/doc_info[id-1]['doc_length'], \n",
    "                  'key' : k\n",
    "                 }\n",
    "            TF_scores.append(temp)\n",
    "    return TF_scores\n",
    "\n",
    "def computeIDF(doc_info, freqDict_list):\n",
    "    IDF_scores = []\n",
    "    counter = 0\n",
    "    for dict in freqDict_list:\n",
    "        counter += 1\n",
    "        for k in dict['freq_dict'].keys():\n",
    "            count = sum([k in tempDict['freq_dict'] for tempDict in freqDict_list])\n",
    "            temp = {'doc_id' : counter, \n",
    "                    'IDF_score' : math.log(len(doc_info)/count), \n",
    "                    'key' : k\n",
    "                   }\n",
    "            IDF_scores.append(temp)\n",
    "    return IDF_scores\n",
    "\n",
    "def computeTFIDF(TF_scores, IDF_scores):\n",
    "    TFIDF_scores = []\n",
    "    for j in IDF_scores:\n",
    "        for i in TF_scores:\n",
    "            if j['key'] == i['key'] and j['doc_id'] == i['doc_id']:\n",
    "                temp = {'doc_id' : i['doc_id'],\n",
    "                        'TFIDF_score' : j['IDF_score']*i['TF_score'],\n",
    "                        'key' : i['key']\n",
    "                       }\n",
    "                TFIDF_scores.append(temp)\n",
    "    return TFIDF_scores\n",
    "\n",
    "doc_info = get_doc(text_sents_clean)\n",
    "\n",
    "freqDict_list = create_freq_dicts(text_sents_clean)\n",
    "\n",
    "TF_Score = computeTF(doc_info, freqDict_list)\n",
    "\n",
    "IDF_Score = computeIDF(doc_info, freqDict_list)\n",
    "\n",
    "TFIDF_Score = computeTFIDF(TF_Score, IDF_Score)\n",
    "\n",
    "with open('tfidf.txt', 'w') as f:\n",
    "    for item in TFIDF_Score:\n",
    "        f.write('%s\\n' % str(item))\n",
    "print(\"Written to tfidf.txt\")\n",
    "\n",
    "print(\"===== POS-tags =====\")\n",
    "\n",
    "pos_output=[]\n",
    "\n",
    "for s in text_sents_clean:\n",
    "    text = word_tokenize(s)\n",
    "    pos_output.append(nltk.pos_tag(text))\n",
    "\n",
    "with open('POS-tags.txt', 'w') as f:\n",
    "    for item in pos_output:\n",
    "        f.write('%s\\n' % str(item))\n",
    "print(\"Written to POS-tags.txt\")\n",
    "\n",
    "print(\"===== Lemmatization (Head Words) =====\")\n",
    "\n",
    "lemma_output = list()\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "\n",
    "for s in text_sents_clean:\n",
    "    sentences = sent_tokenize(s)\n",
    "    for r in sentences:\n",
    "        words = word_tokenize(r)\n",
    "        lemma_output += [ wnl.lemmatize(token.lower()) for token in words ]\n",
    "\n",
    "with open('HeadWords.txt', 'w') as f:\n",
    "    for item in lemma_output:\n",
    "        f.write('%s\\n' % str(item))\n",
    "print(\"Written to HeadWords.txt\")\n",
    "\n",
    "\n",
    "print(\" ========================= \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Due to Memory Constraints, Only Four Categories of News Group are used.\n",
      "===== categories : ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med'] ======\n",
      "===== Loading Partial Training Dataset =====\n",
      "===== Loading Partial Testing Dataset =====\n",
      " ========================= \n",
      "===== Classifiers =====\n",
      "===== Build a MultinomialNB Classifier with Tfidf as Feature =====\n",
      "===== Train the MultinomialNB Classifier with training data =====\n",
      "===== Accuracy of MultinomialNB Classifier =====\n",
      "0.6062134891131173\n",
      "===== Classification Report of MultinomialNB Classifier =====\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.81      0.07      0.13       319\n",
      "           comp.graphics       0.72      0.62      0.67       389\n",
      " comp.os.ms-windows.misc       0.70      0.50      0.59       394\n",
      "comp.sys.ibm.pc.hardware       0.55      0.75      0.64       392\n",
      "   comp.sys.mac.hardware       0.81      0.61      0.69       385\n",
      "          comp.windows.x       0.83      0.74      0.78       395\n",
      "            misc.forsale       0.86      0.69      0.77       390\n",
      "               rec.autos       0.82      0.68      0.74       396\n",
      "         rec.motorcycles       0.89      0.63      0.73       398\n",
      "      rec.sport.baseball       0.95      0.69      0.80       397\n",
      "        rec.sport.hockey       0.59      0.90      0.71       399\n",
      "               sci.crypt       0.47      0.80      0.59       396\n",
      "         sci.electronics       0.77      0.43      0.55       393\n",
      "                 sci.med       0.86      0.63      0.73       396\n",
      "               sci.space       0.84      0.63      0.72       394\n",
      "  soc.religion.christian       0.22      0.95      0.36       398\n",
      "      talk.politics.guns       0.59      0.59      0.59       364\n",
      "   talk.politics.mideast       0.85      0.70      0.77       376\n",
      "      talk.politics.misc       0.81      0.08      0.15       310\n",
      "      talk.religion.misc       0.50      0.00      0.01       251\n",
      "\n",
      "               micro avg       0.61      0.61      0.61      7532\n",
      "               macro avg       0.72      0.58      0.59      7532\n",
      "            weighted avg       0.72      0.61      0.61      7532\n",
      "\n",
      " ========================= \n",
      "===== Build a SGDClassifier with Tfidf as Feature =====\n",
      "===== Train the SGDClassifier with training data =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Accuracy of SGDClassifier =====\n",
      "0.6836165693043016\n",
      "===== Classification Report of SGDClassifier =====\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.56      0.42      0.48       319\n",
      "           comp.graphics       0.69      0.67      0.68       389\n",
      " comp.os.ms-windows.misc       0.67      0.60      0.63       394\n",
      "comp.sys.ibm.pc.hardware       0.65      0.65      0.65       392\n",
      "   comp.sys.mac.hardware       0.76      0.68      0.72       385\n",
      "          comp.windows.x       0.74      0.71      0.73       395\n",
      "            misc.forsale       0.48      0.85      0.61       390\n",
      "               rec.autos       0.79      0.70      0.74       396\n",
      "         rec.motorcycles       0.73      0.77      0.75       398\n",
      "      rec.sport.baseball       0.82      0.78      0.80       397\n",
      "        rec.sport.hockey       0.82      0.91      0.86       399\n",
      "               sci.crypt       0.71      0.74      0.73       396\n",
      "         sci.electronics       0.67      0.49      0.57       393\n",
      "                 sci.med       0.76      0.79      0.78       396\n",
      "               sci.space       0.70      0.76      0.73       394\n",
      "  soc.religion.christian       0.61      0.82      0.70       398\n",
      "      talk.politics.guns       0.56      0.70      0.62       364\n",
      "   talk.politics.mideast       0.74      0.82      0.78       376\n",
      "      talk.politics.misc       0.70      0.35      0.47       310\n",
      "      talk.religion.misc       0.52      0.12      0.20       251\n",
      "\n",
      "               micro avg       0.68      0.68      0.68      7532\n",
      "               macro avg       0.68      0.67      0.66      7532\n",
      "            weighted avg       0.69      0.68      0.67      7532\n",
      "\n",
      " ========================= \n",
      "===== Build a svm.SVC Classifier with Tfidf as Feature =====\n",
      "===== Train the svm.SVC Classifier with training data =====\n",
      "===== Accuracy of svm.SVC Classifier =====\n",
      "0.6631704726500266\n",
      "===== Classification Report of svm.SVC Classifier =====\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.46      0.49      0.47       319\n",
      "           comp.graphics       0.58      0.71      0.64       389\n",
      " comp.os.ms-windows.misc       0.64      0.61      0.62       394\n",
      "comp.sys.ibm.pc.hardware       0.66      0.65      0.65       392\n",
      "   comp.sys.mac.hardware       0.74      0.61      0.67       385\n",
      "          comp.windows.x       0.85      0.64      0.73       395\n",
      "            misc.forsale       0.77      0.79      0.78       390\n",
      "               rec.autos       0.61      0.69      0.65       396\n",
      "         rec.motorcycles       0.46      0.79      0.58       398\n",
      "      rec.sport.baseball       0.75      0.77      0.76       397\n",
      "        rec.sport.hockey       0.91      0.82      0.86       399\n",
      "               sci.crypt       0.84      0.63      0.72       396\n",
      "         sci.electronics       0.55      0.61      0.58       393\n",
      "                 sci.med       0.78      0.69      0.73       396\n",
      "               sci.space       0.66      0.72      0.69       394\n",
      "  soc.religion.christian       0.69      0.74      0.71       398\n",
      "      talk.politics.guns       0.61      0.65      0.63       364\n",
      "   talk.politics.mideast       0.85      0.70      0.77       376\n",
      "      talk.politics.misc       0.55      0.45      0.50       310\n",
      "      talk.religion.misc       0.47      0.26      0.34       251\n",
      "\n",
      "               micro avg       0.66      0.66      0.66      7532\n",
      "               macro avg       0.67      0.65      0.65      7532\n",
      "            weighted avg       0.68      0.66      0.66      7532\n",
      "\n",
      " ========================= \n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "\n",
    "# categories = ['alt.atheism', 'soc.religion.christian',\n",
    "#                'comp.graphics', 'sci.med']\n",
    "\n",
    "# print(\"===== Due to Memory Constraints, Only Four Categories of News Group are used.\")\n",
    "# print(\"===== categories : \"+str(categories)+\" ======\")\n",
    "\n",
    "print(\"===== Loading Training Dataset =====\")\n",
    "twenty_train_partial = fetch_20newsgroups(subset='train', \n",
    "remove=('headers', 'footers', 'quotes'), \n",
    "#                                           categories=categories, \n",
    "                                          shuffle=True, random_state=42)\n",
    "\n",
    "print(\"===== Loading Testing Dataset =====\")\n",
    "twenty_test_partial = fetch_20newsgroups(subset='test', \n",
    "remove=('headers', 'footers', 'quotes'), \n",
    "#                                          categories=categories, \n",
    "                                         shuffle=True, random_state=42)\n",
    "\n",
    "print(\" ========================= \")\n",
    "print(\"===== Classifiers =====\")\n",
    "\n",
    "print(\"===== Build a MultinomialNB Classifier with Tfidf as Feature =====\")\n",
    "text_clf_MultinomialNB = Pipeline([\n",
    "     ('vect', CountVectorizer()),\n",
    "     ('tfidf', TfidfTransformer()),\n",
    "     ('clf', MultinomialNB()),\n",
    " ])\n",
    "\n",
    "print(\"===== Train the MultinomialNB Classifier with training data =====\")\n",
    "text_clf_MultinomialNB.fit(twenty_train_partial.data, twenty_train_partial.target)\n",
    "\n",
    "docs_test_MultinomialNB = twenty_test_partial.data\n",
    "predicted_MultinomialNB = text_clf_MultinomialNB.predict(docs_test_MultinomialNB)\n",
    "\n",
    "print(\"===== Accuracy of MultinomialNB Classifier =====\")\n",
    "print(np.mean(predicted_MultinomialNB == twenty_test_partial.target))\n",
    "\n",
    "print(\"===== Classification Report of MultinomialNB Classifier =====\")\n",
    "print(metrics.classification_report(twenty_test_partial.target, predicted_MultinomialNB,\n",
    "     target_names=twenty_test_partial.target_names))\n",
    "\n",
    "# print(\"===== Confusion Matrix Report of MultinomialNB Classifier =====\")\n",
    "# print(metrics.confusion_matrix(twenty_test_partial.target, predicted_MultinomialNB))\n",
    "\n",
    "print(\" ========================= \")\n",
    "\n",
    "print(\"===== Build a SGDClassifier with Tfidf as Feature =====\")\n",
    "text_clf_SGDClassifier = Pipeline([\n",
    "     ('vect', CountVectorizer()),\n",
    "     ('tfidf', TfidfTransformer()),\n",
    "     ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                          alpha=1e-3, random_state=42,\n",
    "                           max_iter=5, tol=None)),\n",
    " ])\n",
    "\n",
    "print(\"===== Train the SGDClassifier with training data =====\")\n",
    "text_clf_SGDClassifier.fit(twenty_train_partial.data, twenty_train_partial.target)\n",
    "\n",
    "docs_test_SGDClassifier = twenty_test_partial.data\n",
    "predicted_SGDClassifier = text_clf_SGDClassifier.predict(docs_test_SGDClassifier)\n",
    "\n",
    "print(\"===== Accuracy of SGDClassifier =====\")\n",
    "print(np.mean(predicted_SGDClassifier == twenty_test_partial.target))\n",
    "\n",
    "print(\"===== Classification Report of SGDClassifier =====\")\n",
    "print(metrics.classification_report(twenty_test_partial.target, predicted_SGDClassifier,\n",
    "     target_names=twenty_test_partial.target_names))\n",
    "\n",
    "# print(\"===== Confusion Matrix Report of MultinomialNB Classifier =====\")\n",
    "# print(metrics.confusion_matrix(twenty_test_partial.target, predicted_SGDClassifier))\n",
    "\n",
    "print(\" ========================= \")\n",
    "\n",
    "print(\"===== Build a svm.SVC Classifier with Tfidf as Feature =====\")\n",
    "text_clf_svm_SVCClassifier = Pipeline([\n",
    "     ('vect', CountVectorizer()),\n",
    "     ('tfidf', TfidfTransformer()),\n",
    "     ('clf', svm.SVC(kernel='linear')),\n",
    " ])\n",
    "\n",
    "print(\"===== Train the svm.SVC Classifier with training data =====\")\n",
    "text_clf_svm_SVCClassifier.fit(twenty_train_partial.data, twenty_train_partial.target)\n",
    "\n",
    "docs_test_svm_SVCClassifier = twenty_test_partial.data\n",
    "predicted_svm_SVCClassifier = text_clf_svm_SVCClassifier.predict(docs_test_svm_SVCClassifier)\n",
    "\n",
    "print(\"===== Accuracy of svm.SVC Classifier =====\")\n",
    "print(np.mean(predicted_svm_SVCClassifier == twenty_test_partial.target))\n",
    "\n",
    "print(\"===== Classification Report of svm.SVC Classifier =====\")\n",
    "print(metrics.classification_report(twenty_test_partial.target, predicted_svm_SVCClassifier,\n",
    "     target_names=twenty_test_partial.target_names))\n",
    "\n",
    "# print(\"===== Confusion Matrix Report of svm_SVCClassifier Classifier =====\")\n",
    "# print(metrics.confusion_matrix(twenty_test_partial.target, predicted_svm_SVCClassifier))\n",
    "\n",
    "print(\" ========================= \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Due to Memory Constraints, Only Four Categories of News Group are used.\n",
      "===== categories : ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med'] ======\n",
      "===== Loading Partial Dataset =====\n",
      " ========================= \n",
      "===== 10-fold cross validation =====\n",
      "===== Perform 10-fold Cross Validation for SGDClassifier =====\n",
      "Counter({3: 997, 2: 990, 1: 973, 0: 799})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Compute for fold_0 =====\n",
      "accuracy : 0.8351063829787234\n",
      "precision : 0.840069435737129\n",
      "recall : 0.8351063829787234\n",
      "f1_score : 0.8312071110412786\n",
      "===== Compute for fold_1 =====\n",
      "accuracy : 0.8537234042553191\n",
      "precision : 0.8601028409159083\n",
      "recall : 0.8537234042553191\n",
      "f1_score : 0.8494157764861999\n",
      "===== Compute for fold_2 =====\n",
      "accuracy : 0.8404255319148937\n",
      "precision : 0.8491224467018588\n",
      "recall : 0.8404255319148937\n",
      "f1_score : 0.8339858955235389\n",
      "===== Compute for fold_3 =====\n",
      "accuracy : 0.8962765957446809\n",
      "precision : 0.8964394033860396\n",
      "recall : 0.8962765957446809\n",
      "f1_score : 0.8941292199797463\n",
      "===== Compute for fold_4 =====\n",
      "accuracy : 0.848404255319149\n",
      "precision : 0.8556365139283808\n",
      "recall : 0.848404255319149\n",
      "f1_score : 0.846165254879588\n",
      "===== Compute for fold_5 =====\n",
      "accuracy : 0.851063829787234\n",
      "precision : 0.8517083435232738\n",
      "recall : 0.851063829787234\n",
      "f1_score : 0.846085910638123\n",
      "===== Compute for fold_6 =====\n",
      "accuracy : 0.8909574468085106\n",
      "precision : 0.8979289671975622\n",
      "recall : 0.8909574468085106\n",
      "f1_score : 0.8901856674646743\n",
      "===== Compute for fold_7 =====\n",
      "accuracy : 0.875\n",
      "precision : 0.881948592586975\n",
      "recall : 0.875\n",
      "f1_score : 0.8717914366614943\n",
      "===== Compute for fold_8 =====\n",
      "accuracy : 0.8430851063829787\n",
      "precision : 0.8430780884663454\n",
      "recall : 0.8430851063829787\n",
      "f1_score : 0.8378197569785375\n",
      "===== Compute for fold_9 =====\n",
      "accuracy : 0.8613333333333333\n",
      "precision : 0.8703466506971647\n",
      "recall : 0.8613333333333333\n",
      "f1_score : 0.8590648804597704\n",
      " ========================= \n",
      "average accuracy : 0.8595375886524822\n",
      "average precision : 0.8637776448555279\n",
      "average recall : 0.859537110933759\n",
      "average f1 score : 0.8559283614942873\n",
      " ========================= \n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "categories = ['alt.atheism', 'soc.religion.christian',\n",
    "               'comp.graphics', 'sci.med']\n",
    "\n",
    "print(\"===== Due to Memory Constraints, Only Four Categories of News Group are used.\")\n",
    "print(\"===== categories : \"+str(categories)+\" ======\")\n",
    "\n",
    "print(\"===== Loading Partial Dataset =====\")\n",
    "twenty_partial = fetch_20newsgroups(subset='all', \n",
    "remove=('headers', 'footers', 'quotes'), \n",
    "                                          categories=categories, shuffle=True, random_state=42)\n",
    "\n",
    "print(\" ========================= \")\n",
    "print(\"===== 10-fold cross validation =====\")\n",
    "\n",
    "print(\"===== Perform 10-fold Cross Validation for SGDClassifier =====\")\n",
    "\n",
    "def convert_to_np(dataset):\n",
    "    return np.asarray(dataset.data), dataset.target\n",
    "\n",
    "x_train,y_train = convert_to_np(twenty_partial)\n",
    "\n",
    "kf = KFold(n_splits=10)\n",
    "curr_fold = 0\n",
    "acc_list = []\n",
    "folds = []\n",
    "pred_list = []\n",
    "true_list = []\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "print(Counter(y_train))\n",
    "\n",
    "for train_idx, test_idx in kf.split(x_train):\n",
    "    text_clf_KFold = Pipeline([('vect', CountVectorizer()),  # Counts occurrences of each word\n",
    "                         ('tfidf', TfidfTransformer()),  # Normalize the counts based on document length\n",
    "                         ('clf', SGDClassifier(loss='hinge', penalty='l2',  # Call classifier with vector\n",
    "                                               alpha=1e-3, random_state=42,\n",
    "                                               max_iter=5, tol=None)),\n",
    "                         ])\n",
    "    \n",
    "    text_clf_KFold.fit(x_train[train_idx].tolist(), y_train[train_idx])\n",
    "\n",
    "    predicted__KFold = text_clf_KFold.predict(x_train[test_idx])\n",
    "    \n",
    "    print(\"===== Compute for fold_\" + str(curr_fold) + \" =====\")\n",
    "    \n",
    "    acc = accuracy_score(y_train[test_idx].tolist(), predicted__KFold)\n",
    "    acc_list.append(acc)\n",
    "    print(\"accuracy : \" + str(acc))\n",
    "    \n",
    "    prc = precision_score(y_train[test_idx].tolist(), predicted__KFold, average='weighted')\n",
    "    print(\"precision : \" + str(prc))\n",
    "    \n",
    "    rec = recall_score(y_train[test_idx].tolist(), predicted__KFold,  average='weighted')\n",
    "    print(\"recall : \" + str(rec))\n",
    "    \n",
    "    f1 = f1_score(y_train[test_idx].tolist(), predicted__KFold,  average='weighted')\n",
    "    print(\"f1_score : \" + str(f1))\n",
    "    \n",
    "    pred_list+=predicted__KFold.tolist()\n",
    "    true_list+=y_train[test_idx].tolist()\n",
    "    \n",
    "    curr_fold += 1\n",
    "\n",
    "print(\" ========================= \")\n",
    "\n",
    "# av_acc = accuracy_score(true_list, pred_list)\n",
    "av_pre = precision_score(true_list, pred_list, average='weighted')\n",
    "av_rec = recall_score(true_list, pred_list, average='weighted')\n",
    "\n",
    "print(\"average accuracy\" + \" : \" + str(np.average(acc_list)))\n",
    "# print(\"average accuracy\" + \" : \" + str(av_acc))\n",
    "print(\"average precision\" + \" : \" + str(av_pre))\n",
    "print(\"average recall\" + \" : \" + str(av_rec))\n",
    "print(\"average f1 score\" + \" : \" + str(f1_score(true_list, pred_list,  average='weighted')))\n",
    "\n",
    "print(\" ========================= \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
