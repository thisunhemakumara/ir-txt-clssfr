{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\IB\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\IB\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Due to Memory Constraints, Only Four Categories of News Group are used.\n",
      "===== categories : ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med'] ======\n",
      "===== Loading Partial Training Dataset =====\n",
      "===== Loading Partial Testing Dataset =====\n",
      "===== PreProcessing =====\n",
      " ========================= \n",
      " twenty_train_partial.data size : 2257\n",
      " text_sents_clean size : 225\n",
      "===== Features =====\n",
      "===== Bag of Words =====\n",
      "Written to bag_of_words.txt\n",
      "===== N-grams =====\n",
      "Written to n-grams.txt\n",
      "===== TFIDF =====\n",
      "Written to tfidf.txt\n",
      "===== POS-tags =====\n",
      "Written to POS-tags.txt\n",
      "===== Lemmatization (Head Words) =====\n",
      "Written to HeadWords.txt\n",
      " ========================= \n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "import time\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import math\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.util import ngrams\n",
    "\n",
    "categories = ['alt.atheism', 'soc.religion.christian',\n",
    "               'comp.graphics', 'sci.med']\n",
    "\n",
    "print(\"===== Due to Memory Constraints, Only Four Categories of News Group are used.\")\n",
    "print(\"===== categories : \"+str(categories)+\" ======\")\n",
    "\n",
    "print(\"===== Loading Partial Training Dataset =====\")\n",
    "twenty_train_partial = fetch_20newsgroups(subset='train', \n",
    "remove=('headers', 'footers'\n",
    "#         , 'quotes'\n",
    "       ), \n",
    "                                          categories=categories, shuffle=True, random_state=42)\n",
    "\n",
    "print(\"===== Loading Partial Testing Dataset =====\")\n",
    "twenty_test_partial = fetch_20newsgroups(subset='test', \n",
    "remove=('headers', 'footers'\n",
    "#         , 'quotes'\n",
    "       ), \n",
    "                                         categories=categories, shuffle=True, random_state=42)\n",
    "\n",
    "print(\"===== PreProcessing =====\")\n",
    "\n",
    "def remove_string_special_characters(s):\n",
    "    stripped = re.sub('_+', '', s)\n",
    "    stripped = re.sub('\\s+', ' ', stripped)\n",
    "    stripped = re.sub('\\d', '', stripped)\n",
    "    stripped = re.sub('/', ' ', stripped)\n",
    "    stripped = re.sub('[^\\w\\s]', '', stripped)\n",
    "    stripped = stripped.lower()\n",
    "    return stripped\n",
    "\n",
    "text_sents_clean = []\n",
    "\n",
    "def main():\n",
    "    q = multiprocessing.Queue()\n",
    "    for i in range(int(len(twenty_train_partial.data)/10)):\n",
    "        s=remove_string_special_characters(twenty_train_partial.data[i])\n",
    "        text_sents_clean.append(s)\n",
    "        q.put(i)\n",
    "    time.sleep(0.1)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "print(\" ========================= \")\n",
    "print(\" twenty_train_partial.data size : \"+str(len(twenty_train_partial.data)))\n",
    "print(\" text_sents_clean size : \"+str(len(text_sents_clean)))\n",
    "print(\"===== Features =====\")\n",
    "\n",
    "print(\"===== Bag of Words =====\")\n",
    "count_vect = CountVectorizer(analyzer='word', stop_words='english')\n",
    "X_train_counts = count_vect.fit_transform(text_sents_clean)\n",
    "list_bow = list(count_vect.vocabulary_.keys())[:]\n",
    "\n",
    "with open('bag_of_words.txt', 'w') as f:\n",
    "    for item in list_bow:\n",
    "        f.write('%s\\n' % item)\n",
    "print(\"Written to bag_of_words.txt\")\n",
    "\n",
    "print(\"===== N-grams =====\")\n",
    "\n",
    "output = list()\n",
    "\n",
    "for s in text_sents_clean:\n",
    "\n",
    "    tokens = [token for token in s.split(\" \") if token != \"\"]\n",
    "    output += list(ngrams(tokens, 3))\n",
    "\n",
    "with open('n-grams.txt', 'w') as f:\n",
    "    for item in output:\n",
    "        f.write('%s\\n' % str(item))\n",
    "print(\"Written to n-grams.txt\")\n",
    "\n",
    "print(\"===== TFIDF =====\")\n",
    "\n",
    "def get_doc(sent):\n",
    "    doc_info = []\n",
    "    i=0\n",
    "    for sent in text_sents_clean:\n",
    "        i += 1\n",
    "        count=count_words(sent)\n",
    "        temp={'doc_id' : i, 'doc_length' : count }\n",
    "        doc_info.append(temp)\n",
    "    return doc_info\n",
    "\n",
    "def count_words(sent):\n",
    "    count = 0\n",
    "    words = word_tokenize(sent)\n",
    "    for word in words:\n",
    "        count += 1\n",
    "    return count\n",
    "\n",
    "def create_freq_dicts(sents):\n",
    "    i=0\n",
    "    freqDict_list = []\n",
    "    for sent in sents:\n",
    "        i+=1\n",
    "        freq_dict = {}\n",
    "        words = word_tokenize(sent)\n",
    "        for word in words:\n",
    "#             word = word.lower()\n",
    "            if word in freq_dict:\n",
    "                freq_dict[word]=+1\n",
    "            else:\n",
    "                freq_dict[word]=1\n",
    "#             temp={'doc_id' : i, 'freq_dict' : freq_dict }\n",
    "        temp={'doc_id' : i, 'freq_dict' : freq_dict }\n",
    "        freqDict_list.append(temp)\n",
    "    return freqDict_list\n",
    "\n",
    "def computeTF(doc_info, freqDict_list):\n",
    "    TF_scores=[]\n",
    "    for tempDict in freqDict_list:\n",
    "        id=tempDict['doc_id']\n",
    "        for k in tempDict['freq_dict']:\n",
    "            temp={'doc_id' : id, \n",
    "                  'TF_score' : tempDict['freq_dict'][k]/doc_info[id-1]['doc_length'], \n",
    "                  'key' : k\n",
    "                 }\n",
    "            TF_scores.append(temp)\n",
    "    return TF_scores\n",
    "\n",
    "def computeIDF(doc_info, freqDict_list):\n",
    "    IDF_scores = []\n",
    "    counter = 0\n",
    "    for dict in freqDict_list:\n",
    "        counter += 1\n",
    "        for k in dict['freq_dict'].keys():\n",
    "            count = sum([k in tempDict['freq_dict'] for tempDict in freqDict_list])\n",
    "            temp = {'doc_id' : counter, \n",
    "                    'IDF_score' : math.log(len(doc_info)/count), \n",
    "                    'key' : k\n",
    "                   }\n",
    "            IDF_scores.append(temp)\n",
    "    return IDF_scores\n",
    "\n",
    "def computeTFIDF(TF_scores, IDF_scores):\n",
    "    TFIDF_scores = []\n",
    "    for j in IDF_scores:\n",
    "        for i in TF_scores:\n",
    "            if j['key'] == i['key'] and j['doc_id'] == i['doc_id']:\n",
    "                temp = {'doc_id' : i['doc_id'],\n",
    "                        'TFIDF_score' : j['IDF_score']*i['TF_score'],\n",
    "                        'key' : i['key']\n",
    "                       }\n",
    "                TFIDF_scores.append(temp)\n",
    "    return TFIDF_scores\n",
    "\n",
    "doc_info = get_doc(text_sents_clean)\n",
    "\n",
    "freqDict_list = create_freq_dicts(text_sents_clean)\n",
    "\n",
    "TF_Score = computeTF(doc_info, freqDict_list)\n",
    "\n",
    "IDF_Score = computeIDF(doc_info, freqDict_list)\n",
    "\n",
    "TFIDF_Score = computeTFIDF(TF_Score, IDF_Score)\n",
    "\n",
    "with open('tfidf.txt', 'w') as f:\n",
    "    for item in TFIDF_Score:\n",
    "        f.write('%s\\n' % str(item))\n",
    "print(\"Written to tfidf.txt\")\n",
    "\n",
    "print(\"===== POS-tags =====\")\n",
    "\n",
    "pos_output=[]\n",
    "\n",
    "for s in text_sents_clean:\n",
    "    text = word_tokenize(s)\n",
    "    pos_output.append(nltk.pos_tag(text))\n",
    "\n",
    "with open('POS-tags.txt', 'w') as f:\n",
    "    for item in pos_output:\n",
    "        f.write('%s\\n' % str(item))\n",
    "print(\"Written to POS-tags.txt\")\n",
    "\n",
    "print(\"===== Lemmatization (Head Words) =====\")\n",
    "\n",
    "lemma_output = list()\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "\n",
    "for s in text_sents_clean:\n",
    "    sentences = sent_tokenize(s)\n",
    "    for r in sentences:\n",
    "        words = word_tokenize(r)\n",
    "        lemma_output += [ wnl.lemmatize(token.lower()) for token in words ]\n",
    "\n",
    "with open('HeadWords.txt', 'w') as f:\n",
    "    for item in lemma_output:\n",
    "        f.write('%s\\n' % str(item))\n",
    "print(\"Written to HeadWords.txt\")\n",
    "\n",
    "\n",
    "print(\" ========================= \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Due to Memory Constraints, Only Four Categories of News Group are used.\n",
      "===== categories : ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med'] ======\n",
      "===== Loading Partial Training Dataset =====\n",
      "===== Loading Partial Testing Dataset =====\n",
      " ========================= \n",
      "===== Classifiers =====\n",
      "===== Build a MultinomialNB Classifier with Tfidf as Feature =====\n",
      "===== Train the MultinomialNB Classifier with training data =====\n",
      "===== Accuracy of MultinomialNB Classifier =====\n",
      "0.6691078561917443\n",
      "===== Classification Report of MultinomialNB Classifier =====\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "           alt.atheism       0.96      0.08      0.16       319\n",
      "         comp.graphics       0.94      0.83      0.88       389\n",
      "               sci.med       0.96      0.67      0.79       396\n",
      "soc.religion.christian       0.46      0.98      0.63       398\n",
      "\n",
      "             micro avg       0.67      0.67      0.67      1502\n",
      "             macro avg       0.83      0.64      0.61      1502\n",
      "          weighted avg       0.82      0.67      0.63      1502\n",
      "\n",
      " ========================= \n",
      "===== Build a SGDClassifier with Tfidf as Feature =====\n",
      "===== Train the SGDClassifier with training data =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Accuracy of SGDClassifier =====\n",
      "0.7956058588548602\n",
      "===== Classification Report of SGDClassifier =====\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "           alt.atheism       0.80      0.53      0.63       319\n",
      "         comp.graphics       0.77      0.95      0.85       389\n",
      "               sci.med       0.84      0.83      0.83       396\n",
      "soc.religion.christian       0.79      0.83      0.81       398\n",
      "\n",
      "             micro avg       0.80      0.80      0.80      1502\n",
      "             macro avg       0.80      0.78      0.78      1502\n",
      "          weighted avg       0.80      0.80      0.79      1502\n",
      "\n",
      " ========================= \n",
      "===== Build a svm.SVC Classifier with Tfidf as Feature =====\n",
      "===== Train the svm.SVC Classifier with training data =====\n",
      "===== Accuracy of svm.SVC Classifier =====\n",
      "0.8022636484687083\n",
      "===== Classification Report of svm.SVC Classifier =====\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "           alt.atheism       0.76      0.64      0.69       319\n",
      "         comp.graphics       0.79      0.91      0.85       389\n",
      "               sci.med       0.85      0.81      0.83       396\n",
      "soc.religion.christian       0.79      0.82      0.81       398\n",
      "\n",
      "             micro avg       0.80      0.80      0.80      1502\n",
      "             macro avg       0.80      0.79      0.79      1502\n",
      "          weighted avg       0.80      0.80      0.80      1502\n",
      "\n",
      " ========================= \n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "\n",
    "categories = ['alt.atheism', 'soc.religion.christian',\n",
    "               'comp.graphics', 'sci.med']\n",
    "\n",
    "print(\"===== Due to Memory Constraints, Only Four Categories of News Group are used.\")\n",
    "print(\"===== categories : \"+str(categories)+\" ======\")\n",
    "\n",
    "print(\"===== Loading Partial Training Dataset =====\")\n",
    "twenty_train_partial = fetch_20newsgroups(subset='train', \n",
    "remove=('headers', 'footers', 'quotes'), \n",
    "                                          categories=categories, shuffle=True, random_state=42)\n",
    "\n",
    "print(\"===== Loading Partial Testing Dataset =====\")\n",
    "twenty_test_partial = fetch_20newsgroups(subset='test', \n",
    "remove=('headers', 'footers', 'quotes'), \n",
    "                                         categories=categories, shuffle=True, random_state=42)\n",
    "\n",
    "print(\" ========================= \")\n",
    "print(\"===== Classifiers =====\")\n",
    "\n",
    "print(\"===== Build a MultinomialNB Classifier with Tfidf as Feature =====\")\n",
    "text_clf_MultinomialNB = Pipeline([\n",
    "     ('vect', CountVectorizer()),\n",
    "     ('tfidf', TfidfTransformer()),\n",
    "     ('clf', MultinomialNB()),\n",
    " ])\n",
    "\n",
    "print(\"===== Train the MultinomialNB Classifier with training data =====\")\n",
    "text_clf_MultinomialNB.fit(twenty_train_partial.data, twenty_train_partial.target)\n",
    "\n",
    "docs_test_MultinomialNB = twenty_test_partial.data\n",
    "predicted_MultinomialNB = text_clf_MultinomialNB.predict(docs_test_MultinomialNB)\n",
    "\n",
    "print(\"===== Accuracy of MultinomialNB Classifier =====\")\n",
    "print(np.mean(predicted_MultinomialNB == twenty_test_partial.target))\n",
    "\n",
    "print(\"===== Classification Report of MultinomialNB Classifier =====\")\n",
    "print(metrics.classification_report(twenty_test_partial.target, predicted_MultinomialNB,\n",
    "     target_names=twenty_test_partial.target_names))\n",
    "\n",
    "# print(\"===== Confusion Matrix Report of MultinomialNB Classifier =====\")\n",
    "# print(metrics.confusion_matrix(twenty_test_partial.target, predicted_MultinomialNB))\n",
    "\n",
    "print(\" ========================= \")\n",
    "\n",
    "print(\"===== Build a SGDClassifier with Tfidf as Feature =====\")\n",
    "text_clf_SGDClassifier = Pipeline([\n",
    "     ('vect', CountVectorizer()),\n",
    "     ('tfidf', TfidfTransformer()),\n",
    "     ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                          alpha=1e-3, random_state=42,\n",
    "                           max_iter=5, tol=None)),\n",
    " ])\n",
    "\n",
    "print(\"===== Train the SGDClassifier with training data =====\")\n",
    "text_clf_SGDClassifier.fit(twenty_train_partial.data, twenty_train_partial.target)\n",
    "\n",
    "docs_test_SGDClassifier = twenty_test_partial.data\n",
    "predicted_SGDClassifier = text_clf_SGDClassifier.predict(docs_test_SGDClassifier)\n",
    "\n",
    "print(\"===== Accuracy of SGDClassifier =====\")\n",
    "print(np.mean(predicted_SGDClassifier == twenty_test_partial.target))\n",
    "\n",
    "print(\"===== Classification Report of SGDClassifier =====\")\n",
    "print(metrics.classification_report(twenty_test_partial.target, predicted_SGDClassifier,\n",
    "     target_names=twenty_test_partial.target_names))\n",
    "\n",
    "# print(\"===== Confusion Matrix Report of MultinomialNB Classifier =====\")\n",
    "# print(metrics.confusion_matrix(twenty_test_partial.target, predicted_SGDClassifier))\n",
    "\n",
    "print(\" ========================= \")\n",
    "\n",
    "print(\"===== Build a svm.SVC Classifier with Tfidf as Feature =====\")\n",
    "text_clf_svm_SVCClassifier = Pipeline([\n",
    "     ('vect', CountVectorizer()),\n",
    "     ('tfidf', TfidfTransformer()),\n",
    "     ('clf', svm.SVC(kernel='linear')),\n",
    " ])\n",
    "\n",
    "print(\"===== Train the svm.SVC Classifier with training data =====\")\n",
    "text_clf_svm_SVCClassifier.fit(twenty_train_partial.data, twenty_train_partial.target)\n",
    "\n",
    "docs_test_svm_SVCClassifier = twenty_test_partial.data\n",
    "predicted_svm_SVCClassifier = text_clf_svm_SVCClassifier.predict(docs_test_svm_SVCClassifier)\n",
    "\n",
    "print(\"===== Accuracy of svm.SVC Classifier =====\")\n",
    "print(np.mean(predicted_svm_SVCClassifier == twenty_test_partial.target))\n",
    "\n",
    "print(\"===== Classification Report of svm.SVC Classifier =====\")\n",
    "print(metrics.classification_report(twenty_test_partial.target, predicted_svm_SVCClassifier,\n",
    "     target_names=twenty_test_partial.target_names))\n",
    "\n",
    "# print(\"===== Confusion Matrix Report of svm_SVCClassifier Classifier =====\")\n",
    "# print(metrics.confusion_matrix(twenty_test_partial.target, predicted_svm_SVCClassifier))\n",
    "\n",
    "print(\" ========================= \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "categories = ['alt.atheism', 'soc.religion.christian',\n",
    "               'comp.graphics', 'sci.med']\n",
    "\n",
    "print(\"===== Due to Memory Constraints, Only Four Categories of News Group are used.\")\n",
    "print(\"===== categories : \"+str(categories)+\" ======\")\n",
    "\n",
    "print(\"===== Loading Partial Dataset =====\")\n",
    "twenty_partial = fetch_20newsgroups(subset='all', \n",
    "remove=('headers', 'footers', 'quotes'), \n",
    "                                          categories=categories, shuffle=True, random_state=42)\n",
    "\n",
    "print(\" ========================= \")\n",
    "print(\"===== 10-fold cross validation =====\")\n",
    "\n",
    "print(\"===== Perform 10-fold Cross Validation for SGDClassifier =====\")\n",
    "\n",
    "def convert_to_np(dataset):\n",
    "    return np.asarray(dataset.data), dataset.target\n",
    "\n",
    "x_train,y_train = convert_to_np(twenty_partial)\n",
    "\n",
    "kf = KFold(n_splits=10)\n",
    "curr_fold = 0\n",
    "acc_list = []\n",
    "folds = []\n",
    "pred_list = []\n",
    "true_list = []\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "print(Counter(y_train))\n",
    "\n",
    "for train_idx, test_idx in kf.split(x_train):\n",
    "    text_clf_KFold = Pipeline([('vect', CountVectorizer()),  # Counts occurrences of each word\n",
    "                         ('tfidf', TfidfTransformer()),  # Normalize the counts based on document length\n",
    "                         ('clf', SGDClassifier(loss='hinge', penalty='l2',  # Call classifier with vector\n",
    "                                               alpha=1e-3, random_state=42,\n",
    "                                               max_iter=5, tol=None)),\n",
    "                         ])\n",
    "    \n",
    "    text_clf_KFold.fit(x_train[train_idx].tolist(), y_train[train_idx])\n",
    "\n",
    "    predicted__KFold = text_clf_KFold.predict(x_train[test_idx])\n",
    "    \n",
    "    print(\"===== Compute for fold_\" + str(curr_fold) + \" =====\")\n",
    "    \n",
    "    acc = accuracy_score(y_train[test_idx].tolist(), predicted__KFold)\n",
    "    acc_list.append(acc)\n",
    "    print(\"accuracy : \" + str(acc))\n",
    "    \n",
    "    prc = precision_score(y_train[test_idx].tolist(), predicted__KFold, average='weighted')\n",
    "    print(\"precision : \" + str(prc))\n",
    "    \n",
    "    rec = recall_score(y_train[test_idx].tolist(), predicted__KFold,  average='weighted')\n",
    "    print(\"recall : \" + str(rec))\n",
    "    \n",
    "    f1 = f1_score(y_train[test_idx].tolist(), predicted__KFold,  average='weighted')\n",
    "    print(\"f1_score : \" + str(f1))\n",
    "    \n",
    "    pred_list+=predicted__KFold.tolist()\n",
    "    true_list+=y_train[test_idx].tolist()\n",
    "    \n",
    "    curr_fold += 1\n",
    "\n",
    "print(\" ========================= \")\n",
    "\n",
    "# av_acc = accuracy_score(true_list, pred_list)\n",
    "av_pre = precision_score(true_list, pred_list, average='weighted')\n",
    "av_rec = recall_score(true_list, pred_list, average='weighted')\n",
    "\n",
    "print(\"average accuracy\" + \" : \" + str(np.average(acc_list)))\n",
    "# print(\"average accuracy\" + \" : \" + str(av_acc))\n",
    "print(\"average precision\" + \" : \" + str(av_pre))\n",
    "print(\"average recall\" + \" : \" + str(av_rec))\n",
    "print(\"average f1 score\" + \" : \" + str(f1_score(true_list, pred_list,  average='weighted')))\n",
    "\n",
    "print(\" ========================= \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'doc_id': 1, 'TFIDF_score': 0.03150669002545206, 'key': 'naturallanguage'}, {'doc_id': 1, 'TFIDF_score': 0.03150669002545206, 'key': 'processing'}, {'doc_id': 1, 'TFIDF_score': 0.03150669002545206, 'key': 'nlp'}, {'doc_id': 1, 'TFIDF_score': 0.03150669002545206, 'key': 'is'}, {'doc_id': 1, 'TFIDF_score': 0.03150669002545206, 'key': 'an'}, {'doc_id': 1, 'TFIDF_score': 0.03150669002545206, 'key': 'area'}, {'doc_id': 1, 'TFIDF_score': 0.03150669002545206, 'key': 'of'}, {'doc_id': 1, 'TFIDF_score': 0.03150669002545206, 'key': 'computer'}, {'doc_id': 1, 'TFIDF_score': 0.03150669002545206, 'key': 'science'}, {'doc_id': 1, 'TFIDF_score': 0.03150669002545206, 'key': 'and'}, {'doc_id': 1, 'TFIDF_score': 0.03150669002545206, 'key': 'artificial'}, {'doc_id': 1, 'TFIDF_score': 0.03150669002545206, 'key': 'intelligence'}, {'doc_id': 1, 'TFIDF_score': 0.03150669002545206, 'key': 'concerned'}, {'doc_id': 1, 'TFIDF_score': 0.03150669002545206, 'key': 'with'}, {'doc_id': 1, 'TFIDF_score': 0.03150669002545206, 'key': 'the'}, {'doc_id': 1, 'TFIDF_score': 0.03150669002545206, 'key': 'interactions'}, {'doc_id': 1, 'TFIDF_score': 0.03150669002545206, 'key': 'between'}, {'doc_id': 1, 'TFIDF_score': 0.03150669002545206, 'key': 'computers'}, {'doc_id': 1, 'TFIDF_score': 0.03150669002545206, 'key': 'human'}, {'doc_id': 1, 'TFIDF_score': 0.03150669002545206, 'key': 'natural'}, {'doc_id': 1, 'TFIDF_score': 0.03150669002545206, 'key': 'languages'}]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
